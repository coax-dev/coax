FrozenLake with Stochastic Q-Learning
=====================================

In this notebook we solve a non-slippery version of the `FrozenLake-v0
<https://gymnasium.farama.org/environments/toy_text/frozen_lake/>`_ environment using value-based control with q-learning
bootstrap targets.

Instead of learning a point estimate for the expected return, we learn the **distribution** over all
possible returns. This approach is known as Distributional RL, see `paper
<https://arxiv.org/abs/1707.06887>`_.

We'll use a linear function approximator for our learned distribution. Since the observation space
is discrete, this is equivalent to the table-lookup case.

Scroll down to see the plots generated by this script.


----

:download:`stochastic_qlearning.py`

.. image:: https://colab.research.google.com/assets/colab-badge.svg
    :alt: Open in Google Colab
    :target: https://colab.research.google.com/github/coax-dev/coax/blob/main/doc/_notebooks/frozen_lake/stochastic_qlearning.ipynb

.. literalinclude:: stochastic_qlearning.py


.. image:: /_static/img/distributional_rl_frozenlake.png
    :alt: Return distributions, conditioned on (s, a)
    :width: 100%
    :align: center
